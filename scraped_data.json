{
    "url": "https://www.openphilanthropy.org/request-for-proposals-technical-ai-safety-research/",
    "title": "Request for Proposals: Technical AI Safety Research | Open Philanthropy",
    "headings": [
        "Request for Proposals: Technical AI Safety Research",
        "Table of Contents",
        "A Call for AI Safety Research",
        "How to read this RFP",
        "1. Synopsis of research areas",
        "1.1 Adversarial machine learning",
        "1.2 Exploring sophisticated misbehavior in LLMs",
        "1.3 Model transparency",
        "1.4 Trust from first principles",
        "1.5 Alternative approaches to mitigating AI risks",
        "2. Our motivations",
        "2.1 Motivation 1: Safety techniques for near-human-level LLM agents",
        "2.2 Motivation 2: Safety techniques that could scale to superintelligence",
        "3. Application logistics",
        "3.1 Other funding for AI safety research",
        "4. AI safety research areas",
        "Privacy Overview"
    ],
    "paragraphs": [
        "2025, like 2024, will see the release of the most capable AI system in history. In fact, we may see it happen multiple times, each a few weeks or months apart. This won’t require any spectacular breakthroughs — just the same steady progress we’ve seen for the last few years. No one knows how long this trend will last, but many AI researchers and developers now expect we’ll have human-level AI within adecade, and that it will be radicallytransformative.",
        "At Open Philanthropy, we think the possibility oftransformative AIis worth taking seriously and planning for right now. In particular, we should prepare for the risk that AI systems could be misaligned — that they might pursue goals that no one gave them and harm people in the process. We think that ML research today can help to clarify and mitigate the likelihood of this failure mode.",
        "Since 2014, Open Philanthropy has puthundreds of millions of dollarstoward scientific research. We’ve funded groundbreaking work oncomputational protein design, novel methods formalaria eradication, and cutting-edge strategies forpandemicprevention. With transformative AI on the horizon, we see another opportunity for our funding to accelerate highly impactful technical research. In consultation with our technical advisors, we’ve generated a list of research areas that we think offer high leverage for improving our understanding and control of AI.",
        "We expect to spend roughly $40M on this RFP over the next 5 months,and we have funding available to spend substantially more depending on the quality of applications received. We’re open to proposals for grants of many sizes and purposes, ranging from rapid funding for API credits all the way to seed funding for new research organizations.",
        "Whether you’re an expert on one of these research topics or you’ve barely thought about them, we encourage you to apply. Over the last few years, we’ve seen many researchers switch into safety research and produce impactful work, and we think there’s still a lot of ground to cover.",
        "Applications will be open until April 15, 2025.The first step is a 300-word expression of interest; we’ll aim to respond within two weeks to invite strong applicants to write a full proposal.Applications will be evaluated on a rolling basis: the earlier you apply, the earlier we’ll get back to you.",
        "Click Here to Apply",
        "The RFP is organized into four main sections:",
        "We don’t suggest reading the entire document in detail.",
        "If you’re an AI researcher with defined areas of interest:",
        "If you’re new to technical AI safety, or open to exploring multiple areas:",
        "If you’re not a potential applicant, and just want to understand what we’re funding and why, you’ll probably be satisfied with reading parts 1 and 2.",
        "In this section, we briefly orient readers to the 21 research areas that we’ll discuss in more detail onthis page.We expect to fundsomeresearch that doesn’t fall into any of these categories, but will prioritize applications that do fall within them.",
        "These research directions are biased toward areas that our grant evaluators have familiarity with — they are not meant to be a list of the objectively most important or impactful research directions for AI safety, and it’s likely that our prioritization will change over time.For ease of consumption, we’ve grouped them into five rough clusters, though of course there is overlap and ambiguity in how to categorize each research area.",
        "Our favorite topics are marked with a star (*) — we’re especially eager to fund work in these areas. In contrast, we will have a high bar for topics marked with a dagger (†). We encourage applicants considering more than one research area to avoid the “dagger” topics, and ideally choose one of the starred topics.",
        "This cluster of research areas uses simulated red-team/blue-team exercises to expose the vulnerabilities of an LLM (or a system that incorporates LLMs). In these exercises, a blue team attempts to make an AI system adhere with very high reliability to some specification of its safety behavior, and then a red team attempts to find edge cases that violate the specification. We think this adversarial style of evaluation and iteration is necessary to ensure an AI system has a low probability of catastrophic failure. Through these research directions, we aim to develop robust safety techniques that mitigate risks from AIs before those risks emerge in real-world deployments.",
        "Future, more capable AI models might exhibit novel failure modes that are hard to detect with current methods – for instance, failure modes that involve LLMs reasoning about their human developers or becoming optimized to deceive flawed human assessors. We want to fund research that identifies the conditions under which these failure modes occur and makes progress toward robust methods of mitigating or avoiding them.",
        "We see potential in the idea of using a network’s intermediate representations to predict, monitor, or modify its behavior. Some approaches are feasible without an understanding of the model’s learned mechanisms, while other techniques may become possible with the invention of interpretability methods that more comprehensively decompose an AI’s internal mechanisms into components that can be understood and intervened on individually. We’re interested in funding research across this spectrum — everything from useful kludges to new ideas for making models more transparent and steerable.",
        "We trust nuclear power plants and orbital rockets through validated theories that are principled and mechanistic, rather than through direct trial and error. We would benefit from similarly systematic, principled approaches to understanding and predicting AI behavior. One approach to this is model transparency, as in the previous cluster. But understanding may not be a necessary condition: this cluster aims to get the safety and trust benefits of interpretability without humans having to understand any specific AI model in all its details.",
        "These research areas lie outside the scope of the clusters above.",
        "In this section, we outline two scenarios that particularly concern us and shape our choice of research areas. We are interested in funding proposals that help address the threat models and failure modes described here. During the application process, we may ask you to connect your proposal to these motivations.",
        "Today’s cutting-edge LLM agents can match expert performance on discrete benchmarks like the bar exam orGPQA, and even on some realistic open-ended tasks that take humans a fewhours. They still fail at many tasks that human experts can complete in just aday, and are far from being a drop-in replacement for most knowledge workers – but the set of tasks they can perform at human level is growing every few months, and includes more and moreautonomousandgoal-orientedbehavior. Given the rapid pace of AI progress, we think it’s prudent to prepare for the chance that, in just a few years, AI systems could automate the majority of tasks performed by knowledge workers today.",
        "If such advanced AIs are deployed widely and given important influence in economic, governmental, or military activities, then it will become essential to ensure they remain under human control.",
        "While there are a number of risks from AIs in such a future, we’re especially interested in the possibility that an AI might accidentally develop drives, intentions, or goals that conflict with those of its developers and users. This possibility concerns us because AIs may end up in a strong position to accomplish such goals, and we see plausible (but not conclusive) reasons to expect such goals to develop. For example, AIs sometimes takestrategicactionto avoid being turned off or retrained. And at a more basic level, AI developers haveso farfailedto build models that reliably follow their safety specifications. Furthermore, as discussed in the next sectionwe suspect that our current techniques for controlling and aligning AIs will be increasingly inadequate as AIs become more capable.",
        "Despite the speculative nature of this type of misalignment, we think that ML researchers can do work today that clarifies and mitigates these risks. By exploring theworst-case behaviorof today’s AI systems, we can measure and improve our ability to enforce a specification for how AI systems ought to behave. By inducing and studyingconcerning forms of misalignmentin the lab, we can prepare to catch naturally arising misalignment before a disaster. And by building up a more rigorousscience of AI cognition, we can strengthen our methods for shaping and monitoring AIs.",
        "Human-level AI systems would be transformative if widely deployed, but the story won’t end there: AI capabilities are unlikely to plateau at human level. Indeed, human-level AI systems would themselves becapableofconductingAI research and development, potentiallyleadingto a further acceleration in AI capabilities to far beyond human levels. The importance of avoiding undesired behavior by AI systems will accordingly becomeexistential, so these systems will have to be extremely reliable.",
        "Unfortunately, among the techniques that might help makeearlytransformative AI safe, many seem likely to break down once AI capabilities advance far enough. That’s because most of these techniques rely on the AI being unable to completely outsmart us, and being too myopic to pursue long-term objectives far removed from the immediate context of its training. For example, sufficiently capable AI systems could subvert RLHF by reward hacking or by deceiving human overseers (Amodei et al,Ngo et al,Cotra). Similarly, AI control and oversight protocols could fail as systems become increasingly intelligent; some protocols might fail if AI systems become capable of colluding imperceptibly, or capable of reliably distinguishing test scenarios from true opportunities to defect (Greenblatt et al.)",
        "We’d like to fund work that has the potential to lead to safety techniques that remain robust as AI capabilities advance far beyond human level. We are far from confident that any existing techniques have this property, and we don’t think the remaining barriers are likely to be a straightforward engineering challenge. Instead, this seems to require fundamental research toward a more principled theory of the behavior of our AI systems, which will let us confidently estimate how much to trust a given AI as it becomes more capable.",
        "In order to place trust in the alignment of superhuman AI systems, we’ll need to have some means of reliably predicting how models will act in new settings, or at least whether the actions they might take are sufficiently safe. We’re interested in supporting early-stage research that may enable us to eventually produce these sorts of predictions.[1]One important obstacle is that in any field, it often takes many years or even decades for preparadigmatic, fundamental research to yield techniques that can be usefully applied in practice. But even so, we may have adequate time for principled alignment research to make a difference — for …Continue readingThis could involve making the internal structure of AI modelsmore transparentto human understanding, or instead use more theoretical approaches forpredicting how AIs will behave in novel circumstances.",
        "Overview of the application process:",
        "Grants will typically range in size between $50,000 and $5 million. We’re open to considering a wide variety of grant types, including:",
        "This RFP is partly an experiment to inform us about the demand for funding in AI safety research, so please have a low bar for submitting an EOI! It’s okay to submit an EOI even if you’re not sure if you’d accept a grant if we offered it to you. For instance, you could apply for start-up funding for a faculty job even if you think you might go into industry instead.",
        "Applicationswill close on April 15, 2025, at 11:59 PM PDT.",
        "At Open Philanthropy",
        "If you have a proposal that doesn’t fit this RFP, consider applying to ourAI governance RFP, or our recently launchedRFP on Improving Capability Evaluations. If you are an individual at any career stage who would like to pursue a career working on the topics discussed here, you may be a good fit for ourCareer Development and Transition Fundingprogram.",
        "From other sources",
        "For reference, here are some other funders you may be interested in:",
        "Emailaisafety@openphilanthropy.orgif you’re doing AI safety grantmaking and want us to list you here.",
        "The full details of our 21 research areas are available in a separateguide. The guide provides details on each research area, including:",
        "You can find the complete guidehere.",
        "Before applying, please read the descriptions for any areas that interest you, to ensure your application(s) meet the relevant eligibility criteria.",
        "Footnotes[+]Footnotes[−]"
    ]
}